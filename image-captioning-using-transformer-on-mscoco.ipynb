{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anandx05/image-captioning-using-transformer-on-mscoco?scriptVersionId=124183206\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport json\nimport pandas as pd\nimport re\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport collections\nimport random\nimport requests\nimport json\nfrom math import sqrt\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-31T18:32:25.966756Z","iopub.execute_input":"2023-03-31T18:32:25.967487Z","iopub.status.idle":"2023-03-31T18:32:25.97332Z","shell.execute_reply.started":"2023-03-31T18:32:25.967448Z","shell.execute_reply":"2023-03-31T18:32:25.972247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '../input/coco-2017-dataset/coco2017'","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:32:43.787181Z","iopub.execute_input":"2023-03-31T18:32:43.787564Z","iopub.status.idle":"2023-03-31T18:32:43.793149Z","shell.execute_reply.started":"2023-03-31T18:32:43.787529Z","shell.execute_reply":"2023-03-31T18:32:43.791822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(f'{BASE_PATH}/annotations/captions_train2017.json', 'r') as f:\n    data = json.load(f)\n    data = data['annotations']\n\nimg_cap_pairs = []\n\nfor sample in data:\n    img_name = '%012d.jpg' % sample['image_id']\n    img_cap_pairs.append([img_name, sample['caption']])\n\ncaptions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\ncaptions['image'] = captions['image'].apply(\n    lambda x: f'{BASE_PATH}/train2017/{x}'\n)\ncaptions = captions.sample(70000)\ncaptions = captions.reset_index(drop=True)\ncaptions.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:32:46.957923Z","iopub.execute_input":"2023-03-31T18:32:46.958947Z","iopub.status.idle":"2023-03-31T18:32:49.783307Z","shell.execute_reply.started":"2023-03-31T18:32:46.958905Z","shell.execute_reply":"2023-03-31T18:32:49.782256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    text = '[start] ' + text + ' [end]'\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:32:54.425088Z","iopub.execute_input":"2023-03-31T18:32:54.4262Z","iopub.status.idle":"2023-03-31T18:32:54.433398Z","shell.execute_reply.started":"2023-03-31T18:32:54.426159Z","shell.execute_reply":"2023-03-31T18:32:54.431692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions['caption'] = captions['caption'].apply(preprocess)\ncaptions.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:33:01.26163Z","iopub.execute_input":"2023-03-31T18:33:01.262584Z","iopub.status.idle":"2023-03-31T18:33:01.740189Z","shell.execute_reply.started":"2023-03-31T18:33:01.262544Z","shell.execute_reply":"2023-03-31T18:33:01.739063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_row = captions.sample(1).iloc[0]\nprint(random_row.caption)\nprint()\nim = Image.open(random_row.image)\nim","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:33:05.711987Z","iopub.execute_input":"2023-03-31T18:33:05.713145Z","iopub.status.idle":"2023-03-31T18:33:05.857917Z","shell.execute_reply.started":"2023-03-31T18:33:05.713091Z","shell.execute_reply":"2023-03-31T18:33:05.856913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 40\nVOCABULARY_SIZE = 15000\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nEMBEDDING_DIM = 512\nUNITS = 512\nEPOCHS = 1","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:33:37.994987Z","iopub.execute_input":"2023-03-31T18:33:37.995985Z","iopub.status.idle":"2023-03-31T18:33:38.00209Z","shell.execute_reply.started":"2023-03-31T18:33:37.995929Z","shell.execute_reply":"2023-03-31T18:33:38.000634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = tf.keras.layers.TextVectorization(\n    max_tokens=VOCABULARY_SIZE,\n    standardize=None,\n    output_sequence_length=MAX_LENGTH)\n\ntokenizer.adapt(captions['caption'])","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:33:42.852936Z","iopub.execute_input":"2023-03-31T18:33:42.853647Z","iopub.status.idle":"2023-03-31T18:33:48.187278Z","shell.execute_reply.started":"2023-03-31T18:33:42.853608Z","shell.execute_reply":"2023-03-31T18:33:48.186066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.vocabulary_size()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:33:51.454361Z","iopub.execute_input":"2023-03-31T18:33:51.454761Z","iopub.status.idle":"2023-03-31T18:33:51.462734Z","shell.execute_reply.started":"2023-03-31T18:33:51.454726Z","shell.execute_reply":"2023-03-31T18:33:51.461413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\npickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:33:56.028544Z","iopub.execute_input":"2023-03-31T18:33:56.028913Z","iopub.status.idle":"2023-03-31T18:33:56.125501Z","shell.execute_reply.started":"2023-03-31T18:33:56.028876Z","shell.execute_reply":"2023-03-31T18:33:56.124483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2idx = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary())\n\nidx2word = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary(),\n    invert=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:00.213702Z","iopub.execute_input":"2023-03-31T18:34:00.214399Z","iopub.status.idle":"2023-03-31T18:34:00.499796Z","shell.execute_reply.started":"2023-03-31T18:34:00.214356Z","shell.execute_reply":"2023-03-31T18:34:00.498756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(captions['image'], captions['caption']):\n    img_to_cap_vector[img].append(cap)\n\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_keys = (img_keys[:slice_index], \n                                          img_keys[slice_index:])\n\ntrain_imgs = []\ntrain_captions = []\nfor imgt in img_name_train_keys:\n    capt_len = len(img_to_cap_vector[imgt])\n    train_imgs.extend([imgt] * capt_len)\n    train_captions.extend(img_to_cap_vector[imgt])\n\nval_imgs = []\nval_captions = []\nfor imgv in img_name_val_keys:\n    capv_len = len(img_to_cap_vector[imgv])\n    val_imgs.extend([imgv] * capv_len)\n    val_captions.extend(img_to_cap_vector[imgv])","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:06.512545Z","iopub.execute_input":"2023-03-31T18:34:06.512956Z","iopub.status.idle":"2023-03-31T18:34:06.748693Z","shell.execute_reply.started":"2023-03-31T18:34:06.512918Z","shell.execute_reply":"2023-03-31T18:34:06.747563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:11.107887Z","iopub.execute_input":"2023-03-31T18:34:11.10894Z","iopub.status.idle":"2023-03-31T18:34:11.116432Z","shell.execute_reply.started":"2023-03-31T18:34:11.108896Z","shell.execute_reply":"2023-03-31T18:34:11.115208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(img_path, caption):\n    img = tf.io.read_file(img_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    caption = tokenizer(caption)\n    return img, caption","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:16.300645Z","iopub.execute_input":"2023-03-31T18:34:16.30101Z","iopub.status.idle":"2023-03-31T18:34:16.307643Z","shell.execute_reply.started":"2023-03-31T18:34:16.300975Z","shell.execute_reply":"2023-03-31T18:34:16.306151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (train_imgs, train_captions))\n\ntrain_dataset = train_dataset.map(\n    load_data, num_parallel_calls=tf.data.AUTOTUNE\n    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    (val_imgs, val_captions))\n\nval_dataset = val_dataset.map(\n    load_data, num_parallel_calls=tf.data.AUTOTUNE\n    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:20.525959Z","iopub.execute_input":"2023-03-31T18:34:20.526914Z","iopub.status.idle":"2023-03-31T18:34:21.287111Z","shell.execute_reply.started":"2023-03-31T18:34:20.52686Z","shell.execute_reply":"2023-03-31T18:34:21.286056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_augmentation = tf.keras.Sequential(\n    [\n        tf.keras.layers.RandomFlip(\"horizontal\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomContrast(0.3),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:26.74721Z","iopub.execute_input":"2023-03-31T18:34:26.747922Z","iopub.status.idle":"2023-03-31T18:34:26.760356Z","shell.execute_reply.started":"2023-03-31T18:34:26.747882Z","shell.execute_reply":"2023-03-31T18:34:26.758885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CNN_Encoder():\n    inception_v3 = tf.keras.applications.InceptionV3(\n        include_top=False,\n        weights='imagenet'\n    )\n\n    output = inception_v3.output\n    output = tf.keras.layers.Reshape(\n        (-1, output.shape[-1]))(output)\n\n    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n    return cnn_model","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:31.288222Z","iopub.execute_input":"2023-03-31T18:34:31.288595Z","iopub.status.idle":"2023-03-31T18:34:31.295419Z","shell.execute_reply.started":"2023-03-31T18:34:31.288558Z","shell.execute_reply":"2023-03-31T18:34:31.294038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n        self.attention = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n    \n\n    def call(self, x, training):\n        x = self.layer_norm_1(x)\n        x = self.dense(x)\n\n        attn_output = self.attention(\n            query=x,\n            value=x,\n            key=x,\n            attention_mask=None,\n            training=training\n        )\n\n        x = self.layer_norm_2(x + attn_output)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:35.545493Z","iopub.execute_input":"2023-03-31T18:34:35.545891Z","iopub.status.idle":"2023-03-31T18:34:35.554864Z","shell.execute_reply.started":"2023-03-31T18:34:35.545856Z","shell.execute_reply":"2023-03-31T18:34:35.553704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Embeddings(tf.keras.layers.Layer):\n\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embeddings = tf.keras.layers.Embedding(\n            vocab_size, embed_dim)\n        self.position_embeddings = tf.keras.layers.Embedding(\n            max_len, embed_dim, input_shape=(None, max_len))\n    \n\n    def call(self, input_ids):\n        length = tf.shape(input_ids)[-1]\n        position_ids = tf.range(start=0, limit=length, delta=1)\n        position_ids = tf.expand_dims(position_ids, axis=0)\n\n        token_embeddings = self.token_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n\n        return token_embeddings + position_embeddings","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:40.732609Z","iopub.execute_input":"2023-03-31T18:34:40.732971Z","iopub.status.idle":"2023-03-31T18:34:40.742194Z","shell.execute_reply.started":"2023-03-31T18:34:40.732937Z","shell.execute_reply":"2023-03-31T18:34:40.740414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self, embed_dim, units, num_heads):\n        super().__init__()\n        self.embedding = Embeddings(\n            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n\n        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n\n        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n\n        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n\n        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n\n        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n    \n\n    def call(self, input_ids, encoder_output, training, mask=None):\n        embeddings = self.embedding(input_ids)\n\n        combined_mask = None\n        padding_mask = None\n        \n        if mask is not None:\n            causal_mask = self.get_causal_attention_mask(embeddings)\n            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n            combined_mask = tf.minimum(combined_mask, causal_mask)\n\n        attn_output_1 = self.attention_1(\n            query=embeddings,\n            value=embeddings,\n            key=embeddings,\n            attention_mask=combined_mask,\n            training=training\n        )\n\n        out_1 = self.layernorm_1(embeddings + attn_output_1)\n\n        attn_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_output,\n            key=encoder_output,\n            attention_mask=padding_mask,\n            training=training\n        )\n\n        out_2 = self.layernorm_2(out_1 + attn_output_2)\n\n        ffn_out = self.ffn_layer_1(out_2)\n        ffn_out = self.dropout_1(ffn_out, training=training)\n        ffn_out = self.ffn_layer_2(ffn_out)\n\n        ffn_out = self.layernorm_3(ffn_out + out_2)\n        ffn_out = self.dropout_2(ffn_out, training=training)\n        preds = self.out(ffn_out)\n        return preds\n\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0\n        )\n        return tf.tile(mask, mult)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:45.233302Z","iopub.execute_input":"2023-03-31T18:34:45.233677Z","iopub.status.idle":"2023-03-31T18:34:45.269104Z","shell.execute_reply.started":"2023-03-31T18:34:45.233645Z","shell.execute_reply":"2023-03-31T18:34:45.267303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageCaptioningModel(tf.keras.Model):\n\n    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n        super().__init__()\n        self.cnn_model = cnn_model\n        self.encoder = encoder\n        self.decoder = decoder\n        self.image_aug = image_aug\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n\n\n    def calculate_loss(self, y_true, y_pred, mask):\n        loss = self.loss(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n\n    def calculate_accuracy(self, y_true, y_pred, mask):\n        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n        accuracy = tf.math.logical_and(mask, accuracy)\n        accuracy = tf.cast(accuracy, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n    \n\n    def compute_loss_and_acc(self, img_embed, captions, training=True):\n        encoder_output = self.encoder(img_embed, training=True)\n        y_input = captions[:, :-1]\n        y_true = captions[:, 1:]\n        mask = (y_true != 0)\n        y_pred = self.decoder(\n            y_input, encoder_output, training=True, mask=mask\n        )\n        loss = self.calculate_loss(y_true, y_pred, mask)\n        acc = self.calculate_accuracy(y_true, y_pred, mask)\n        return loss, acc\n\n    \n    def train_step(self, batch):\n        imgs, captions = batch\n\n        if self.image_aug:\n            imgs = self.image_aug(imgs)\n        \n        img_embed = self.cnn_model(imgs)\n\n        with tf.GradientTape() as tape:\n            loss, acc = self.compute_loss_and_acc(\n                img_embed, captions\n            )\n    \n        train_vars = (\n            self.encoder.trainable_variables + self.decoder.trainable_variables\n        )\n        grads = tape.gradient(loss, train_vars)\n        self.optimizer.apply_gradients(zip(grads, train_vars))\n        self.loss_tracker.update_state(loss)\n        self.acc_tracker.update_state(acc)\n\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n    \n\n    def test_step(self, batch):\n        imgs, captions = batch\n\n        img_embed = self.cnn_model(imgs)\n\n        loss, acc = self.compute_loss_and_acc(\n            img_embed, captions, training=False\n        )\n\n        self.loss_tracker.update_state(loss)\n        self.acc_tracker.update_state(acc)\n\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.acc_tracker]","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:34:53.54793Z","iopub.execute_input":"2023-03-31T18:34:53.54835Z","iopub.status.idle":"2023-03-31T18:34:53.566541Z","shell.execute_reply.started":"2023-03-31T18:34:53.548316Z","shell.execute_reply":"2023-03-31T18:34:53.565033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\ndecoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n\ncnn_model = CNN_Encoder()\ncaption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:35:04.233052Z","iopub.execute_input":"2023-03-31T18:35:04.234099Z","iopub.status.idle":"2023-03-31T18:35:07.34907Z","shell.execute_reply.started":"2023-03-31T18:35:04.233998Z","shell.execute_reply":"2023-03-31T18:35:07.347896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=\"none\"\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\ncaption_model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=cross_entropy\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:35:14.050148Z","iopub.execute_input":"2023-03-31T18:35:14.051157Z","iopub.status.idle":"2023-03-31T18:35:14.070429Z","shell.execute_reply.started":"2023-03-31T18:35:14.051105Z","shell.execute_reply":"2023-03-31T18:35:14.069159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = caption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T13:41:51.729758Z","iopub.execute_input":"2023-03-31T13:41:51.73011Z","iopub.status.idle":"2023-03-31T16:17:44.55609Z","shell.execute_reply.started":"2023-03-31T13:41:51.730082Z","shell.execute_reply":"2023-03-31T16:17:44.54786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\n\ncaption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=[early_stopping]\n    )\n\nfor i, log in enumerate(early_stopping.stopped_epochs):\n    print(f'Epoch {i+1} stopped at: {log}')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T10:48:03.277676Z","iopub.execute_input":"2023-04-01T10:48:03.278126Z","iopub.status.idle":"2023-04-01T10:48:13.574016Z","shell.execute_reply.started":"2023-04-01T10:48:03.278087Z","shell.execute_reply":"2023-04-01T10:48:13.572048Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2966967971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m caption_model.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'caption_model' is not defined"],"ename":"NameError","evalue":"name 'caption_model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='validation loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T19:12:24.432297Z","iopub.execute_input":"2023-03-31T19:12:24.432664Z","iopub.status.idle":"2023-03-31T19:12:24.667131Z","shell.execute_reply.started":"2023-03-31T19:12:24.432631Z","shell.execute_reply":"2023-03-31T19:12:24.666053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_from_path(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img\n\n\ndef generate_caption(img_path, add_noise=False):\n    img = load_image_from_path(img_path)\n    \n    if add_noise:\n        noise = tf.random.normal(img.shape)*0.1\n        img = img + noise\n        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n    \n    img = tf.expand_dims(img, axis=0)\n    img_embed = caption_model.cnn_model(img)\n    img_encoded = caption_model.encoder(img_embed, training=False)\n\n    y_inp = '[start]'\n    for i in range(MAX_LENGTH-1):\n        tokenized = tokenizer([y_inp])[:, :-1]\n        mask = tf.cast(tokenized != 0, tf.int32)\n        pred = caption_model.decoder(\n            tokenized, img_encoded, training=False, mask=mask)\n        \n        pred_idx = np.argmax(pred[0, i, :])\n        pred_idx = tf.convert_to_tensor(pred_idx)\n        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n        if pred_word == '[end]':\n            break\n        \n        y_inp += ' ' + pred_word\n    \n    y_inp = y_inp.replace('[start] ', '')\n    return y_inp","metadata":{"execution":{"iopub.status.busy":"2023-03-31T19:12:29.605361Z","iopub.execute_input":"2023-03-31T19:12:29.605918Z","iopub.status.idle":"2023-03-31T19:12:29.620557Z","shell.execute_reply.started":"2023-03-31T19:12:29.605859Z","shell.execute_reply":"2023-03-31T19:12:29.619177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = random.randrange(0, len(captions))\nimg_path = captions.iloc[idx].image\n\npred_caption = generate_caption(img_path)\nprint('Predicted Caption:', pred_caption)\nprint()\nImage.open(img_path)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T19:12:33.46383Z","iopub.execute_input":"2023-03-31T19:12:33.464797Z","iopub.status.idle":"2023-03-31T19:12:35.86645Z","shell.execute_reply.started":"2023-03-31T19:12:33.464742Z","shell.execute_reply":"2023-03-31T19:12:35.865402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#img_url = \"https://images.squarespace-cdn.com/content/v1/5e0e65adcd39ed279a0402fd/1627422658456-7QKPXTNQ34W2OMBTESCJ/1.jpg?format=2500w\"\n#img_url = \"https://tensorflow.org/images/surf.jpg\" #a man riding a surf board on a wave\n#img_url = \"https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg\" #two bed repeated\n#img_url = \"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AtxxyM5bg2WEGh0rySAMXQ.jpeg\"\n#img_url = \"https://miro.medium.com/v2/resize:fit:524/format:webp/1*tVETPw5HjXoNM9Ne5DSZWA.jpeg\"\n#img_url = \"https://www.thesprucepets.com/thmb/3ABKoAPm0Hu4PcWsDH1giawq7ck=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg\"\n#img_url = \"https://cdn.britannica.com/79/232779-050-6B0411D7/German-Shepherd-dog-Alsatian.jpg?w=400&h=300&c=crop\"\nimg_url = \"https://cdn.theatlantic.com/thumbor/viW9N1IQLbCrJ0HMtPRvXPXShkU=/0x131:2555x1568/976x549/media/img/mt/2017/06/shutterstock_319985324/original.jpg\"\n\n\n\n\n\n\n\n\nim = Image.open(requests.get(img_url, stream=True).raw)\nim = im.convert('RGB')\nim.save('tmp.jpg')\n\npred_caption = generate_caption('tmp.jpg', add_noise=False)\nprint('Predicted Caption:', pred_caption)\nprint()\nim","metadata":{"execution":{"iopub.status.busy":"2023-03-31T19:12:40.427898Z","iopub.execute_input":"2023-03-31T19:12:40.42893Z","iopub.status.idle":"2023-03-31T19:12:43.138735Z","shell.execute_reply.started":"2023-03-31T19:12:40.428869Z","shell.execute_reply":"2023-03-31T19:12:43.137714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model.save_weights('model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-03-31T16:31:36.303703Z","iopub.execute_input":"2023-03-31T16:31:36.304108Z","iopub.status.idle":"2023-03-31T16:31:37.082836Z","shell.execute_reply.started":"2023-03-31T16:31:36.304071Z","shell.execute_reply":"2023-03-31T16:31:37.081331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel_path = '/kaggle/working/model.h5'\nmodel = load_model(model_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:03:45.00339Z","iopub.execute_input":"2023-03-31T18:03:45.003791Z","iopub.status.idle":"2023-03-31T18:03:45.030876Z","shell.execute_reply.started":"2023-03-31T18:03:45.003756Z","shell.execute_reply":"2023-03-31T18:03:45.029184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have already built the model architecture\ncaption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)\n\n\n# Load the saved weights\nsaved_weights_path = \"../kaggle/working/model.h5\"\ncaption_model.load_weights(saved_weights_path)\n\n# Compile the model with the same optimizer, loss function, and metrics\ncaption_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Continue training the model\ncaption_model.fit(train_dataset, epochs=additional_epochs, validation_data=val_dataset)\n\n# Save the updated weights (optional)\n#model.save_weights(\"../output/kaggle/working/model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-03-31T17:38:21.642397Z","iopub.execute_input":"2023-03-31T17:38:21.642837Z","iopub.status.idle":"2023-03-31T17:38:21.683492Z","shell.execute_reply.started":"2023-03-31T17:38:21.642797Z","shell.execute_reply":"2023-03-31T17:38:21.681678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncaption_model.load_weights('../output/kaggle/working/model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T17:22:08.402708Z","iopub.execute_input":"2023-03-31T17:22:08.403284Z","iopub.status.idle":"2023-03-31T17:22:08.432587Z","shell.execute_reply.started":"2023-03-31T17:22:08.403247Z","shell.execute_reply":"2023-03-31T17:22:08.430078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\n\n# Load the COCO dataset\nannotations_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\nimg_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n\nwith open(annotations_file, 'r') as f:\n    annotations = json.load(f)\n\ncaptions = []\nfor annot in annotations['annotations']:\n    caption = '<start> ' + annot['caption'] + ' <end>'\n    img_id = annot['image_id']\n    file_name = 'COCO_train2014_' + str(img_id).zfill(12) + '.jpg'\n    captions.append({'image': img_dir + file_name, 'caption': caption})\n\ncaptions = pd.DataFrame(captions)\n\n# Split the data into training and validation sets\ntrain_size = int(len(captions) * 0.8)\ntrain_captions = captions[:train_size]\nval_captions = captions[train_size:]\n\n# Define the maximum vocabulary size\nVOCAB_SIZE = 10000\n\n# Define the tokenizer\ntokenizer = tf.keras.preprocessing.text.Tokenizer(\n    num_words=VOCAB_SIZE, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\n)\ntokenizer.fit_on_texts(train_captions['caption'])\n\n\n# Define the datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_captions['image'], train_captions['caption']))\ntrain_dataset = train_dataset.map(load_image_and_caption, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_captions['image'], val_captions['caption']))\nval_dataset = val_dataset.map(load_image_and_caption, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Define the image augmentation layer\nimage_augmentation = tf.keras.Sequential(\n    [\n        tf.keras.layers.RandomFlip(\"horizontal\"),\n        tf.keras.layers.RandomRotation(0.2),\n        tf.keras.layers.RandomContrast(0.3),\n    ]\n)\n\n# Define the model architecture\nencoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\ndecoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n\ncnn_model = CNN_Encoder()\ncaption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)\n\n# Load the saved weights\ncaption_model.load_weights('/kaggle/working/model.h5')\n\n# Compile the model\ncaption_model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=cross_entropy\n)\n\n# Train the model for additional epochs\nhistory = caption_model.fit(\n    train_dataset,\n    epochs=1,\n    validation_data=val_dataset,\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:26:56.01198Z","iopub.execute_input":"2023-03-31T18:26:56.012365Z","iopub.status.idle":"2023-03-31T18:27:06.428578Z","shell.execute_reply.started":"2023-03-31T18:26:56.012334Z","shell.execute_reply":"2023-03-31T18:27:06.425514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model = ImageCaptioningModel(cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation)\n# Load the saved weights\ncaption_model.load_weights('/kaggle/working/model.h5')\n\n# Compile the model\ncaption_model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=cross_entropy\n)\n\n# Train the model for additional epochs\nhistory = caption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=[early_stopping]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T18:49:40.228151Z","iopub.execute_input":"2023-03-31T18:49:40.228573Z","iopub.status.idle":"2023-03-31T18:49:40.267305Z","shell.execute_reply.started":"2023-03-31T18:49:40.228536Z","shell.execute_reply":"2023-03-31T18:49:40.265725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"history = caption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=val_dataset,\n    callbacks=[early_stopping]\n)","metadata":{}}]}